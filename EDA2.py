"""
Legal RAG Project - Final Ultimate EDA Script

[ì„¤ì • ìš”ì•½]
- Log Scale ì ìš©: 1ë²ˆ(ë¬¸ì„œ ê¸¸ì´), 3ë²ˆ(í† í° ìˆ˜) -> ê·¹ë‹¨ê°’ ë³´ì •
- Linear Scale (ì›ë³¸): ë‚˜ë¨¸ì§€ 6ê°œ -> ë°ì´í„° ê·¸ëŒ€ë¡œ í‘œí˜„
- ê°œì„ ì‚¬í•­: ë§‰ëŒ€ ê·¸ë˜í”„ ìœ„ì— ì •í™•í•œ ìˆ˜ì¹˜(Label) í‘œê¸°
- ê²½ê³  í•´ê²°: Palette/Hue Warning, Glyph Warning í•´ê²°

Target: Full Dataset
Theme: Earth Tones (Extended 20 Colors)
"""

import json
import os
import sys
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from collections import Counter, defaultdict
from tqdm import tqdm
from wordcloud import WordCloud
from transformers import AutoTokenizer

# -----------------------------------------------------------------------------
# ğŸ¨ Design System: Earth & Beige (20ìƒ‰ í™•ì¥íŒ)
# -----------------------------------------------------------------------------
COLORS = {
    'bg': '#F9F8F2',
    'text': '#4A4238',
    'sub_text': '#8C857B',
    'accent_safe': '#8FBC8F',
    'accent_risk': '#E2725B',
    'palette': [
        '#8FBC8F', '#D2B48C', '#CD853F', '#778899', '#BC8F8F', '#E2725B',
        '#A9A9A9', '#556B2F', '#8B4513', '#DAA520', '#5F9EA0', '#A0522D',
        '#6B8E23', '#BDB76B', '#4682B4', '#DEB887', '#2F4F4F', '#CD5C5C',
        '#808000', '#708090'
    ]
}

FONT_PATH = None

def set_plot_style():
    plt.rcParams['figure.facecolor'] = COLORS['bg']
    plt.rcParams['axes.facecolor'] = COLORS['bg']
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['axes.unicode_minus'] = False
    
    # í°íŠ¸ ìë™ ì„¤ì •
    import platform
    global FONT_PATH
    system_name = platform.system()
    if system_name == 'Darwin':
        plt.rcParams['font.family'] = 'AppleGothic'
        FONT_PATH = '/System/Library/Fonts/Supplemental/AppleGothic.ttf'
    elif system_name == 'Windows':
        plt.rcParams['font.family'] = 'Malgun Gothic'
        FONT_PATH = 'C:/Windows/Fonts/malgun.ttf'
    
    sns.set_palette(sns.color_palette(COLORS['palette']))

# -----------------------------------------------------------------------------
# ğŸ§  Final Analysis Class
# -----------------------------------------------------------------------------

class FinalLegalEDA:
    def __init__(self, data_path, result_dir='eda_results'):
        self.data_path = data_path
        self.result_dir = Path(result_dir)
        self.result_dir.mkdir(parents=True, exist_ok=True)
        self.docs = self._load_data()

    def _load_data(self):
        print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘: {self.data_path}")
        if not os.path.exists(self.data_path):
            print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
            
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        documents = data.get('data', data) if isinstance(data, dict) else data
        print(f"âœ… ì´ {len(documents):,}ê±´ì˜ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ.\n")
        return documents

    # =========================================================================
    # Part 1. Basic Analysis (ê¸°ì´ˆ)
    # =========================================================================

    def analyze_01_length(self):
        """[1] ë¬¸ì„œ ê¸¸ì´ ë¶„ì„ (Log Scale)"""
        print("[1/8] ë¬¸ì„œ ê¸¸ì´ ë¶„ì„ ì¤‘ (Log Scale ì ìš©)...")
        lengths = [len(doc.get('text', '')) for doc in self.docs]
        
        plt.figure(figsize=(10, 6))
        # log_scale=True ì ìš©
        sns.histplot(lengths, bins=50, color=COLORS['palette'][1], kde=True, log_scale=True)
        plt.axvline(np.mean(lengths), color=COLORS['text'], linestyle='--', label=f'Mean: {np.mean(lengths):.0f}')
        plt.title('ë¬¸ì„œ ê¸¸ì´ ë¶„í¬ (Log Scale)', fontsize=15)
        plt.xlabel('ê¸€ì ìˆ˜ (Log Scale)')
        plt.legend()
        plt.tight_layout()
        plt.savefig(self.result_dir / '01_basic_length.png', dpi=300)

    def analyze_02_structure(self):
        """[2] ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ (Linear Scale + ìˆ˜ì¹˜ í‘œì‹œ)"""
        print("[2/8] ë¬¸ì„œ êµ¬ì¡° íŒ¨í„´ ë¶„ì„ ì¤‘...")
        pattern = r'ã€(.*?)ã€‘'
        all_headers = []
        
        # ë…¸ì´ì¦ˆ í•„í„°ë§ (ë„ˆë¬´ ê¸´ í—¤ë” ì œì™¸)
        for doc in tqdm(self.docs, desc="Scanning"):
            headers = re.findall(pattern, doc.get('text', ''))
            all_headers.extend([h.strip() for h in headers if len(h) < 10])
            
        counts = Counter(all_headers).most_common(15)
        df = pd.DataFrame(counts, columns=['Section', 'Count'])
        
        plt.figure(figsize=(12, 6))
        # Warning í•´ê²°: hue ì§€ì •, legend=False
        ax = sns.barplot(data=df, x='Section', y='Count', hue='Section', palette=COLORS['palette'], legend=False)
        
        # ë§‰ëŒ€ ìœ„ì— ìˆ«ì í‘œì‹œ (ì§„ì§œ ë˜‘ê°™ì€ì§€ í™•ì¸ìš©)
        for i in ax.containers:
            ax.bar_label(i, fmt='%d', padding=3)
            
        plt.title('ì£¼ìš” ë¬¸ì„œ ì„¹ì…˜ í—¤ë” (Top 15)', fontsize=15)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(self.result_dir / '02_basic_structure.png', dpi=300)

    def analyze_03_tokens(self):
        """[3] í† í° ìˆ˜ ë¶„ì„ (Log Scale)"""
        print("[3/8] í† í° ìˆ˜ ë¶„ì„ ì¤‘ (Log Scale ì ìš©)...")
        try:
            tokenizer = AutoTokenizer.from_pretrained("jhgan/ko-sroberta-multitask")
            use_tokenizer = True
        except:
            use_tokenizer = False
            
        token_counts = []
        for doc in tqdm(self.docs, desc="Counting"):
            text = doc.get('text', '')
            if use_tokenizer:
                token_counts.append(len(tokenizer.encode(text, add_special_tokens=False)))
            else:
                token_counts.append(len(text.split()))
                
        plt.figure(figsize=(10, 6))
        # log_scale=True ì ìš©
        sns.histplot(token_counts, bins=50, color=COLORS['palette'][3], kde=True, log_scale=True)
        plt.axvline(512, color=COLORS['accent_risk'], linestyle='--', label='512 Tokens')
        plt.title('í† í° ìˆ˜ ë¶„í¬ (Log Scale)', fontsize=15)
        plt.xlabel('í† í° ìˆ˜ (Log Scale)')
        plt.legend()
        plt.tight_layout()
        plt.savefig(self.result_dir / '03_basic_tokens.png', dpi=300)

    def analyze_04_wordcloud(self):
        """[4] ì›Œë“œ í´ë¼ìš°ë“œ"""
        print("[4/8] ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± ì¤‘...")
        keywords = []
        for doc in self.docs:
            keywords.extend(doc.get('keyword', []))
        if not keywords: return

        count = Counter(keywords)
        wc = WordCloud(
            font_path=FONT_PATH, width=1200, height=800,
            background_color=COLORS['bg'], colormap='copper',
            max_words=100
        ).generate_from_frequencies(count)
        
        plt.figure(figsize=(12, 8))
        plt.imshow(wc, interpolation='bilinear')
        plt.axis('off')
        plt.title('ì£¼ìš” ë²•ë¥  í‚¤ì›Œë“œ', fontsize=16)
        plt.tight_layout()
        plt.savefig(self.result_dir / '04_basic_wordcloud.png', dpi=300)

    # =========================================================================
    # Part 2. Advanced Analysis (ì‹¬í™”)
    # =========================================================================

    def analyze_05_ne_distribution(self):
        """[5] ê°œì²´ëª…(NE) ë¶„í¬"""
        print("[5/8] ê°œì²´ëª…(NE) ë¶„í¬ ë¶„ì„ ì¤‘...")
        types = []
        for doc in self.docs:
            if 'NE' in doc:
                types.extend([ne['type'] for ne in doc['NE']])
        if not types: return
        
        df = pd.DataFrame(Counter(types).items(), columns=['Type', 'Count']).sort_values('Count', ascending=False)
        
        plt.figure(figsize=(12, 6))
        ax = sns.barplot(data=df, x='Count', y='Type', hue='Type', palette=COLORS['palette'], legend=False)
        
        # ìˆ˜ì¹˜ í‘œì‹œ
        for i in ax.containers:
            ax.bar_label(i, fmt='%d', padding=3)
            
        plt.title('ê°œì²´ëª…(NE) íƒ€ì…ë³„ ë¶„í¬', fontsize=15)
        plt.tight_layout()
        plt.savefig(self.result_dir / '05_adv_ne_dist.png', dpi=300)

    def analyze_06_risk(self):
        """[6] ì²­í‚¹ ìœ„í—˜ë„"""
        print("[6/8] ì²­í‚¹ ìœ„í—˜ë„ ë¶„ì„ ì¤‘...")
        risk_ratios = []
        for doc in tqdm(self.docs, desc="Risk Calc"):
            text_len = len(doc.get('text', ''))
            if text_len == 0: continue
            ne_len = sum([ne['end'] - ne['begin'] for ne in doc.get('NE', [])])
            risk_ratios.append((ne_len / text_len) * 100)
            
        avg_risk = np.mean(risk_ratios)
        
        plt.figure(figsize=(10, 6))
        sns.histplot(risk_ratios, bins=50, color=COLORS['accent_risk'], kde=True)
        plt.axvline(avg_risk, color=COLORS['text'], linestyle='--', label=f'Mean Risk: {avg_risk:.1f}%')
        plt.title('ë¬¸ì„œë³„ ì²­í‚¹ ìœ„í—˜ë„ ë¶„í¬ (NE Ratio)', fontsize=15)
        plt.xlabel('ìœ„í—˜ë„ (%)')
        plt.legend()
        plt.tight_layout()
        plt.savefig(self.result_dir / '06_adv_risk.png', dpi=300)

    def analyze_07_heatmap(self):
        """[7] ì—”í‹°í‹° íˆíŠ¸ë§µ (ìˆ˜ì •ë¨: Float -> Int ë³€í™˜ ì¶”ê°€)"""
        print("[7/8] ë²•ë ¹-íŒê²° ê´€ê³„ ë¶„ì„ ì¤‘...")
        pair_counts = defaultdict(int)
        stop_laws = ['í—Œë²•', 'ë¯¼ë²•', 'í˜•ë²•'] # ë„ˆë¬´ í”í•œ ë²•ë¥  ì œì™¸
        
        for doc in tqdm(self.docs, desc="Mapping"):
            laws = set([ne['entity'] for ne in doc.get('NE', []) if ne['type'] == 'CV_LAW'])
            judgments = set([ne['entity'] for ne in doc.get('NE', []) if ne['type'] == 'TML_JUDGMENT'])
            
            for l in laws:
                if l in stop_laws or len(l) < 2: continue
                for j in judgments:
                    if len(j) < 2: continue
                    short_l = l[:10] + '..' if len(l) > 10 else l
                    short_j = j[:6] + '..' if len(j) > 6 else j
                    pair_counts[(short_l, short_j)] += 1
                    
        if not pair_counts: return
        
        top_pairs = sorted(pair_counts.items(), key=lambda x: x[1], reverse=True)[:20]
        
        # Pivot Tableë¡œ ë³€í™˜
        data = [{'Law': k[0], 'Judgment': k[1], 'Count': v} for k, v in top_pairs]
        df = pd.DataFrame(data)
        
        # 1. í”¼ë²— í…Œì´ë¸” ìƒì„±
        matrix = df.pivot_table(index='Law', columns='Judgment', values='Count', fill_value=0)
        
        # 2. [í•µì‹¬ ìˆ˜ì •] ì‹¤ìˆ˜(float)ë¥¼ ì •ìˆ˜(int)ë¡œ ê°•ì œ ë³€í™˜
        matrix = matrix.astype(int)
        
        plt.figure(figsize=(10, 8))
        # ì´ì œ ë°ì´í„°ê°€ intì´ë¯€ë¡œ fmt='d'ê°€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤
        sns.heatmap(matrix, annot=True, fmt='d', cmap='OrRd')
        plt.title('ì£¼ìš” ë²•ë ¹-íŒê²° ì—°ê´€ì„± (Filtered)', fontsize=15)
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(self.result_dir / '07_adv_heatmap.png', dpi=300)

    def analyze_08_grounding(self):
        """[8] ê·¸ë¼ìš´ë”©"""
        print("[8/8] í‚¤ì›Œë“œ ê·¸ë¼ìš´ë”© ê²€ì¦ ì¤‘...")
        match, miss = 0, 0
        for doc in tqdm(self.docs, desc="Grounding"):
            text = doc.get('text', '')
            for kw in doc.get('keyword', []):
                if kw in text: match += 1
                else: miss += 1
        total = match + miss
        if total == 0: return
        
        plt.figure(figsize=(6, 6))
        plt.pie([match, miss], labels=['Matched', 'Missed'], 
                colors=[COLORS['accent_safe'], COLORS['accent_risk']],
                autopct='%1.1f%%', startangle=90)
        plt.title(f'í‚¤ì›Œë“œ-ë³¸ë¬¸ ì¼ì¹˜ìœ¨', fontsize=15)
        plt.tight_layout()
        plt.savefig(self.result_dir / '08_adv_grounding.png', dpi=300)

    def run_all(self):
        print("="*60)
        print("ğŸš€ Final Ultimate EDA Starting...")
        print("="*60)
        
        self.analyze_01_length()
        self.analyze_02_structure()
        self.analyze_03_tokens()
        self.analyze_04_wordcloud()
        self.analyze_05_ne_distribution()
        self.analyze_06_risk()
        self.analyze_07_heatmap()
        self.analyze_08_grounding()
        
        print("\nâœ¨ ëª¨ë“  ë¶„ì„ ì™„ë£Œ! 'eda_results' í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    set_plot_style()
    # ì‹¤ì œ ë°ì´í„° ê²½ë¡œ ì„¤ì •
    DATA_PATH = 'data/Training/02.ë¼ë²¨ë§ë°ì´í„°/Training_legal.json'
    eda = FinalLegalEDA(DATA_PATH)
    eda.run_all()