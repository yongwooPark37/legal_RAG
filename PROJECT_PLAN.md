# 프로젝트 실험 계획 및 접근 방법

## EDA 결과 요약

### 데이터셋 개요
- **Training 데이터**: 63,704개 문서
- **Validation 데이터**: 7,963개 문서
- **평균 텍스트 길이**: 약 2,980 문자 (약 2,359자 중앙값)
- **텍스트 길이 범위**: 55 ~ 133,165 문자

### 주요 발견사항

#### 1. NER (Named Entity Recognition) 분석
- **총 엔티티 수**: 약 277만개 (Training 데이터)
- **고유 엔티티 타입**: 10개
- **문서당 평균 엔티티 수**: 약 43.58개

**엔티티 타입 분포 (상위 5개)**:
1. `CV_LAW` (법률 개념): 1,083,609개 (39.0%)
2. `TML_PROVISION` (법조문): 406,240개 (14.6%)
3. `TML_PARTY` (당사자): 376,201개 (13.5%)
4. `OGG_LAW` (법률 기관): 300,117개 (10.8%)
5. `TML_PROCEDURAL_ACTS` (절차적 행위): 167,990개 (6.0%)

#### 2. 카테고리 분포
- **고유 카테고리**: 23개
- **주요 카테고리** (상위 5개):
  1. 민사소송법: 10,315개 (16.19%)
  2. 형법: 9,731개 (15.28%)
  3. 부동산/건설사업법: 9,362개 (14.70%)
  4. 조세/세법: 8,875개 (13.93%)
  5. 노동법: 7,617개 (11.96%)

#### 3. 키워드 분석
- **총 키워드**: 311,379개
- **고유 키워드**: 80,332개
- **문서당 평균 키워드**: 4.89개
- **상위 키워드**: 대법원, 판결, 소유권, 계약, 법원 등

#### 4. 시간적 분포
- **연도 범위**: 1987 ~ 2022
- **최근 데이터**: 2021-2022년에 집중 (약 28,386개, 44.5%)

## RAG 시스템 설계 방향

### 1. 데이터 특성 기반 고려사항

#### 텍스트 길이 문제
- 평균 2,980자로 상당히 긴 문서
- RAG에서 chunking 전략이 중요:
  - **문장 단위 분할**: 법률 판례의 구조적 특성 활용
  - **의미적 단위 분할**: 판시사항, 판결요지, 참조조문 등 섹션 기반
  - **Overlap 전략**: 중요한 법률 개념이 누락되지 않도록

#### NER 정보 활용
- 풍부한 NER 라벨링 정보 활용 가능:
  - 엔티티 기반 검색 강화
  - 법률 개념, 조문, 판례 간 연결성 파악
  - 엔티티 임베딩을 활용한 의미 검색

#### 카테고리 기반 필터링
- 23개 법률 카테고리 정보 활용:
  - 카테고리별 특화 검색
  - 카테고리 기반 재랭킹
  - 도메인 특화 임베딩 모델 고려

### 2. 실험 가능한 모델 및 접근 방법

#### A. 임베딩 모델 실험
1. **한국어 특화 모델**:
   - `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
   - `jhgan/ko-sroberta-multitask`
   - `BM-K/KoSimCSE-roberta-multitask`
   - `sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens`

2. **법률 도메인 특화**:
   - 법률 데이터로 fine-tuning한 모델
   - NER 정보를 활용한 지도 학습 임베딩

3. **대규모 모델**:
   - BERT 계열 (한국어 BERT)
   - T5 계열 (한국어 T5)
   - KoBERT, KoELECTRA

#### B. LLM 모델 실험
1. **Open-source 모델**:
   - Llama 2/3 (한국어 fine-tuned)
   - Mistral
   - Qwen (중국어 지원, 한국어도 가능)
   - Polyglot-Ko

2. **API 기반 모델**:
   - GPT-4, GPT-3.5-turbo
   - Claude (Anthropic)
   - Gemini (Google)

#### C. RAG 구조 개선

1. **Retrieval 전략**:
   - **Dense Retrieval**: 임베딩 기반 벡터 검색
   - **Sparse Retrieval**: BM25, TF-IDF
   - **Hybrid Retrieval**: Dense + Sparse 조합
   - **Reranking**: Cross-encoder 기반 재랭킹

2. **Chunking 전략**:
   - 고정 길이 chunking
   - 법률 문서 구조 기반 chunking (판시사항, 판결요지 등)
   - 의미 단위 chunking (NER 기반)

3. **Augmentation Prompting**:
   - **Few-shot prompting**: 유사 판례 예시 제공
   - **Chain-of-Thought**: 단계별 추론 유도
   - **Self-consistency**: 여러 답변 생성 후 일치도 확인
   - **Role-based prompting**: 법률 전문가 역할 설정

### 3. 차별화 전략 제안

#### 전략 1: NER 기반 의미 검색 강화
- **아이디어**: NER 정보를 활용한 다중 레벨 검색
  - 일반 텍스트 검색 + 엔티티 검색
  - 엔티티 간 관계 그래프 구축
  - 법률 개념 간 유사도 기반 검색

#### 전략 2: 법률 카테고리 기반 도메인 특화
- **아이디어**: 카테고리별 특화된 RAG 파이프라인
  - 카테고리별 별도 벡터 스토어
  - 카테고리 분류 모델과 RAG 결합
  - 카테고리별 프롬프트 템플릿

#### 전략 3: 시간적 맥락 고려
- **아이디어**: 판례의 시간적 특성 활용
  - 최신 판례 우선 검색
  - 법률 개정에 따른 판례 중요도 조정
  - 시간 기반 재랭킹

#### 전략 4: 멀티모달 검색 (문서 구조 활용)
- **아이디어**: 판례의 구조적 정보 활용
  - 판시사항, 판결요지, 참조조문 등 섹션별 검색
  - 구조 정보를 메타데이터로 활용
  - 섹션별 중요도 가중치

#### 전략 5: Augmentation Prompting 고도화
- **아이디어**: 법률 도메인 특화 프롬프트 엔지니어링
  - 법률 논리 구조에 맞는 프롬프트 설계
  - 판례 비교 프롬프트
  - 법조문 해석 프롬프트
  - 케이스 기반 추론 프롬프트

### 4. 평가 메트릭

#### Retrieval 평가
- **Recall@K**: 상위 K개 검색 결과 중 관련 문서 포함률
- **MRR (Mean Reciprocal Rank)**: 첫 번째 관련 문서의 역순위 평균
- **NDCG@K**: 정규화된 누적 이득

#### Generation 평가
- **BLEU**: 참조 답변과의 유사도
- **ROUGE**: 요약 품질 평가
- **BERTScore**: 의미적 유사도 평가
- **법률 전문가 평가**: 실제 법률 전문가의 정확도 평가

#### End-to-end 평가
- **질문 답변 정확도**: 법률 질문에 대한 답변 정확도
- **판례 검색 정확도**: 관련 판례 찾기 정확도
- **법조문 해석 정확도**: 법조문 해석의 정확도

## 다음 단계 액션 아이템

### Phase 1: Baseline 구축 (1-2주)
1. ✅ EDA 완료
2. ⏳ 기본 RAG 파이프라인 구축
   - 단순 임베딩 모델 (예: sentence-transformers)
   - FAISS 또는 ChromaDB 벡터 스토어
   - 기본 chunking 전략
   - GPT-3.5 또는 오픈소스 LLM 연동

3. ⏳ Baseline 평가
   - 검색 정확도 측정
   - 생성 품질 평가

### Phase 2: 실험 및 개선 (2-3주)
4. ⏳ 다양한 임베딩 모델 실험
5. ⏳ Chunking 전략 실험
6. ⏳ Augmentation Prompting 실험
7. ⏳ NER 정보 활용 실험
8. ⏳ 카테고리 기반 필터링 실험

### Phase 3: 차별화 접근 (2-3주)
9. ⏳ 고유한 접근 방법 구현
   - 위에서 제안한 차별화 전략 중 선택
10. ⏳ 성능 개선 및 최적화
11. ⏳ 최종 평가 및 분석

### Phase 4: 문서화 및 발표 (1주)
12. ⏳ 결과 분석 및 보고서 작성
13. ⏳ 발표 준비

## 참고사항

- **데이터 특성**: 법률 판례는 전문적이고 구조화된 텍스트
- **도메인 지식**: 법률 용어와 개념의 정확한 이해 중요
- **평가의 어려움**: 법률 도메인은 정답이 명확하지 않은 경우가 많음
- **실용성**: 실제 사용 가능한 수준의 정확도 필요

